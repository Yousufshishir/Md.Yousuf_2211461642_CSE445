{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24055281",
   "metadata": {},
   "source": [
    "# Project-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys, subprocess, importlib, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to install missing packages automatically\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        return importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"[INFO] Installing missing package: {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        return importlib.import_module(pkg)\n",
    "\n",
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ensure(\"xgboost\")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility function that saves figures in \"output_images/\" folder\n",
    "# ------------------------------------------------------------\n",
    "OUT_DIR = \"output_images\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def savefig(path, dpi=220, bbox_inches=\"tight\"):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, path), dpi=dpi, bbox_inches=bbox_inches)\n",
    "    print(f\"     Saved: {os.path.join(OUT_DIR, path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a0e20",
   "metadata": {},
   "source": [
    "## Step 1: Load the collected CSV data\n",
    "\n",
    "I have collected data  and stored them in a CSV file.\n",
    "Now, I load it into pandas for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tries to find the CSV file automatically from common paths\n",
    "candidates = [\n",
    "    \"stroop_test_data.csv\",\n",
    "    \"./data/stroop_test_data.csv\",\n",
    "    \"/mnt/data/stroop_test_data.csv\"\n",
    "]\n",
    "csv_path = None\n",
    "for p in candidates:\n",
    "    if os.path.exists(p):\n",
    "        csv_path = p\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"Please place 'stroop_test_data.csv' in the project folder.\")\n",
    "\n",
    "print(f\"[INFO] Loaded data from: {csv_path}\")\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24263d",
   "metadata": {},
   "source": [
    "## Step 2: Clean and normalize the data\n",
    "\n",
    "I clean the dataset by:\n",
    "- Normalizing column names\n",
    "- Removing duplicates\n",
    "- Converting values to numeric\n",
    "- Clipping invalid values (like negative accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# This auto-detects accuracy and reaction-time columns\n",
    "col_map = {\"accuracy\": None, \"reaction_time\": None}\n",
    "for c in df.columns:\n",
    "    if col_map[\"accuracy\"] is None and (\"accuracy\" in c and \"percent\" in c):\n",
    "        col_map[\"accuracy\"] = c\n",
    "    if col_map[\"reaction_time\"] is None and ((\"reaction\" in c and \"time\" in c) or c in {\"rt\",\"avg_rt\",\"mean_rt\"}):\n",
    "        col_map[\"reaction_time\"] = c\n",
    "\n",
    "acc_col = col_map[\"accuracy\"]\n",
    "rt_col  = col_map[\"reaction_time\"]\n",
    "\n",
    "# Drops duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Converts to numeric\n",
    "df[acc_col] = pd.to_numeric(df[acc_col], errors=\"coerce\")\n",
    "df[rt_col]  = pd.to_numeric(df[rt_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[acc_col, rt_col]).reset_index(drop=True)\n",
    "\n",
    "# Clip values\n",
    "df[acc_col] = df[acc_col].clip(0,100)\n",
    "df[rt_col]  = df[rt_col].clip(lower=1e-6)\n",
    "\n",
    "print(f\"[INFO] Final dataset shape after cleaning: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279ead4",
   "metadata": {},
   "source": [
    "## Step 3: Manual labeling\n",
    "\n",
    "I have labeled the data manually using a **rule-based approach**:\n",
    "- If accuracy is very high (â‰¥ 80th percentile), or\n",
    "- If accuracy is above median and reaction time is faster than median  \n",
    "ðŸ‘‰ Then I label as **High Performer (1)**. Otherwise **Low Performer (0)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates median and 80th percentile for accuracy and reaction time\n",
    "acc_median = df[acc_col].median()\n",
    "acc_p80    = df[acc_col].quantile(0.80)\n",
    "rt_median  = df[rt_col].median()\n",
    "\n",
    "def make_label(row):\n",
    "    acc = row[acc_col]\n",
    "    rt  = row[rt_col]\n",
    "    if (acc >= acc_p80) or (acc >= acc_median and rt <= rt_median):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df[\"label\"] = df.apply(make_label, axis=1)\n",
    "print(df[\"label\"].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170ddd4",
   "metadata": {},
   "source": [
    "## Step 4: Redundant feature detection\n",
    "\n",
    "I have checked for:\n",
    "- **Low variance features** (columns with almost constant values)\n",
    "- **Highly correlated features** (correlation > 0.95)\n",
    "\n",
    "These are dropped to avoid redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this selects numeric columns and identify features to drop based on variance and correlation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c != \"label\"]\n",
    "\n",
    "# Low variance\n",
    "low_variance_cols = [c for c in feature_cols if df[c].nunique() <= 1]\n",
    "\n",
    "# High correlation\n",
    "corr = df[feature_cols].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr_pairs = [(c1, c2) for c1 in upper.columns for c2 in upper.columns if (upper.loc[c1,c2] > 0.95)]\n",
    "\n",
    "# Features to drop\n",
    "drop_for_corr = {c2 for (c1,c2) in high_corr_pairs}\n",
    "to_drop = list(set(low_variance_cols) | drop_for_corr)\n",
    "kept_features = [c for c in feature_cols if c not in to_drop]\n",
    "\n",
    "print(\"[INFO] Dropped features:\", to_drop)\n",
    "print(\"[INFO] Kept features:\", kept_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbde26f",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test split and scaling\n",
    "\n",
    "I split the data into training and testing sets (80/20).\n",
    "I also scale the features for Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for training\n",
    "X = df[kept_features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(f\"[INFO] Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b0412",
   "metadata": {},
   "source": [
    "## Step 6: Model Training\n",
    "\n",
    "I trained three models:\n",
    "1. **Neural Network (MLPClassifier)**\n",
    "2. **Random Forest**\n",
    "3. **XGBoost**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eadecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "nn = MLPClassifier(hidden_layer_sizes=(16,8), activation=\"relu\", max_iter=1000, random_state=42)\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb_clf = XGBClassifier(n_estimators=400, learning_rate=0.07, max_depth=3,\n",
    "                        subsample=0.9, colsample_bytree=0.9, random_state=42, eval_metric=\"logloss\")\n",
    "xgb_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bdc4c",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation\n",
    "\n",
    "I have shown **Confusion Matrices** and **Classification Reports** for all three models.  \n",
    "All images will be automatically saved in the `output_images/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_nn  = nn.predict(X_test_scaled)\n",
    "y_pred_rf  = rf.predict(X_test)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "print(\"Neural Network Report:\\n\", classification_report(y_test, y_pred_nn))\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"XGBoost Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Confusion matrix plotting\n",
    "def plot_and_save_cm(y_true, y_pred, title, fname):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
    "    disp.plot(ax=ax, colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "plot_and_save_cm(y_test, y_pred_nn,  \"Neural Network CM\", \"NN_CM.jpg\")\n",
    "plot_and_save_cm(y_test, y_pred_rf,  \"Random Forest CM\", \"RF_CM.jpg\")\n",
    "plot_and_save_cm(y_test, y_pred_xgb, \"XGBoost CM\", \"XGB_CM.jpg\")\n",
    "\n",
    "# Combined Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14,4))\n",
    "for ax, (title, preds) in zip(axes, [(\"NN\",y_pred_nn), (\"RF\",y_pred_rf), (\"XGB\",y_pred_xgb)]):\n",
    "    cm = confusion_matrix(y_test, preds, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "    disp.plot(ax=ax, colorbar=False)\n",
    "    ax.set_title(title)\n",
    "savefig(\"Confusion_Matrix.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89bf91",
   "metadata": {},
   "source": [
    "## Step 8: Data Visualization\n",
    "\n",
    "Finally, I created a scatter plot of the collected data.\n",
    "This shows **accuracy vs reaction time**, colored by manual labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7883aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "for label_val, label_name in [(0, \"Low\"), (1, \"High\")]:\n",
    "    mask = (df[\"label\"] == label_val)\n",
    "    plt.scatter(df.loc[mask, acc_col], df.loc[mask, rt_col],\n",
    "                alpha=0.8, label=f\"{label_name} performer\")\n",
    "\n",
    "plt.xlabel(acc_col)\n",
    "plt.ylabel(rt_col)\n",
    "plt.title(\"Collected Data Visualization\")\n",
    "plt.legend()\n",
    "plt.grid(True, ls=\"--\", alpha=0.4)\n",
    "savefig(\"Data_Visualize.jpg\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
